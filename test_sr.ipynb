{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models.srcnn import *\n",
    "from models.vdsr import *\n",
    "\n",
    "from models.sr_model import *\n",
    "from models.edrn import *\n",
    "from models.vdsr import *\n",
    "from models.srresnet_ import *\n",
    "from models.sr_model import *\n",
    "from models.vdsr import *\n",
    "from models.utils import *\n",
    "from models.srcnn import *\n",
    "from models.edsr import *\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edsr_srcnn = EDSR_srcnnfy()\n",
    "edsr = EDSR().to(device)\n",
    "edrn_canny = EDRN(use_canny=True).to(device)\n",
    "edrn_sobel = EDRN(use_sobel=True).to(device)\n",
    "srresnet = SRResNet().to(device)\n",
    "vdsr = VDSR().to(device)\n",
    "srcnn = SRCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149750/957710577.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edsr.load_state_dict(torch.load('best_edsr.pth', map_location=device))\n",
      "/tmp/ipykernel_149750/957710577.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edrn_sobel.load_state_dict(torch.load('weight_pcb/best_sobel_srx4_model.pth', map_location=device))\n",
      "/tmp/ipykernel_149750/957710577.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edrn_canny.load_state_dict(torch.load('weight_pcb/best_canny_srx4_model.pth', map_location=device))\n",
      "/tmp/ipykernel_149750/957710577.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  srresnet.load_state_dict(torch.load('best_srresnet.pth', map_location=device))\n",
      "/tmp/ipykernel_149750/957710577.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vdsr.load_state_dict(torch.load('weight_pcb/best_vdsr.pth', map_location=device))\n",
      "/tmp/ipykernel_149750/957710577.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  srcnn.load_state_dict(torch.load('weight_pcb/best_srcnn.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "edsr.load_state_dict(torch.load('best_edsr.pth', map_location=device))\n",
    "edrn_sobel.load_state_dict(torch.load('weight_pcb/best_sobel_srx4_model.pth', map_location=device))\n",
    "edrn_canny.load_state_dict(torch.load('weight_pcb/best_canny_srx4_model.pth', map_location=device))\n",
    "srresnet.load_state_dict(torch.load('best_srresnet.pth', map_location=device))\n",
    "vdsr.load_state_dict(torch.load('weight_pcb/best_vdsr.pth', map_location=device))\n",
    "srcnn.load_state_dict(torch.load('weight_pcb/best_srcnn.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "def calculate_metrics(img1, img2, max_pixel_value=1.0):\n",
    "    psnr = PeakSignalNoiseRatio(data_range=1.0)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "    psnr_value = psnr(img1, img2)\n",
    "    ssim_value = ssim(img1, img2)\n",
    "    return psnr_value, ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, type =False):\n",
    "        self.lr_files = sorted(os.listdir(lr_dir))\n",
    "        self.hr_files = sorted(os.listdir(hr_dir))\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.type =type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image = cv2.imread(os.path.join(self.lr_dir, self.lr_files[idx]))\n",
    "        hr_image = cv2.imread(os.path.join(self.hr_dir, self.hr_files[idx]))\n",
    "        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
    "        hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)\n",
    "        if self.type:\n",
    "            height, width = lr_image.shape[:2]\n",
    "            scale_factor = 4\n",
    "            # lr_height, lr_width = height // 2, width // 2\n",
    "            hr_height, hr_width = height * 4, width * 4\n",
    "\n",
    "            # # Rescale ·∫£nh LR l√™n 4 l·∫ßn\n",
    "            # lr_image = cv2.resize(lr_image, (lr_width, lr_height), interpolation=cv2.INTER_CUBIC)\n",
    "            lr_image = cv2.resize(lr_image, (hr_width, hr_height), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "        # hr_image = cv2.resize(hr_image, (hr_width, hr_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        lr_image = transform(lr_image)\n",
    "        hr_image = transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'benchmark/DIV2K/LR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m valid_hr_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenchmark/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/HR\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m output_image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenchmark/output/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_lr_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_hr_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m valid_dataset_2 \u001b[38;5;241m=\u001b[39m ImageDataset(valid_lr_dir, valid_hr_dir, \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_dataset)\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mImageDataset.__init__\u001b[0;34m(self, lr_dir, hr_dir, type)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lr_dir, hr_dir, \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhr_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(hr_dir))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_dir \u001b[38;5;241m=\u001b[39m lr_dir\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'benchmark/DIV2K/LR'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "sub = 'DIV2K'\n",
    "valid_lr_dir = f'benchmark/{sub}/LR'\n",
    "valid_hr_dir = f'benchmark/{sub}/HR'\n",
    "output_image_dir = f'benchmark/output/{sub}'\n",
    "valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir)\n",
    "valid_dataset_2 = ImageDataset(valid_lr_dir, valid_hr_dir, type = True)\n",
    "valid_loader = DataLoader(valid_dataset)\n",
    "valid_loader_2 = DataLoader(valid_dataset_2)\n",
    "log_file = open('subtracttion.txt', 'a')\n",
    "log_file.write(f'{sub}\\n')\n",
    "# ƒêo th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho t·ª´ng m√¥ h√¨nh\n",
    "sobel_total_time = 0\n",
    "canny_total_time = 0\n",
    "edsr_total_time = 0\n",
    "srresnet_total_time = 0\n",
    "vdsr_total_time = 0\n",
    "srcnn_total_time = 0\n",
    "bicubic_total_time = 0\n",
    "\n",
    "edrn_sobel.eval()\n",
    "edrn_canny.eval()\n",
    "srresnet.eval()\n",
    "srcnn.eval()\n",
    "edsr.eval()\n",
    "vdsr.eval()\n",
    "\n",
    "val_psnr_values_bicubic = 0\n",
    "val_psnr_values_sobel = 0\n",
    "val_psnr_values_canny = 0\n",
    "val_psnr_values_edsr = 0\n",
    "val_psnr_values_srresnet = 0\n",
    "val_psnr_values_vdsr = 0\n",
    "val_psnr_values_srcnn = 0\n",
    "\n",
    "val_ssim_values_sobel = 0\n",
    "val_ssim_values_canny = 0\n",
    "val_ssim_values_edsr = 0\n",
    "val_ssim_values_srresnet = 0\n",
    "val_ssim_values_vdsr = 0\n",
    "val_ssim_values_srcnn = 0\n",
    "val_ssim_values_bicubic = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():  # Kh√¥ng t√≠nh to√°n gradient trong qu√° tr√¨nh validation\n",
    "    for (lr_images, hr_images) in tqdm(valid_loader_2, unit='batch'):\n",
    "        # VDSR validation\n",
    "        start_time = time.time()\n",
    "        outputs_vdsr = vdsr(lr_images)\n",
    "        vdsr_total_time += time.time() - start_time\n",
    "        psnr_vdsr, ssim_vdsr = calculate_metrics(outputs_vdsr, hr_images)\n",
    "\n",
    "        # SRCNN validation\n",
    "        start_time = time.time()\n",
    "        outputs_srcnn = srcnn(lr_images)\n",
    "        srcnn_total_time += time.time() - start_time\n",
    "        psnr_srcnn, ssim_srcnn = calculate_metrics(outputs_srcnn, hr_images)\n",
    "\n",
    "        # C·∫≠p nh·∫≠t PSNR v√† SSIM cho VDSR v√† SRCNN\n",
    "        val_psnr_values_vdsr += psnr_vdsr\n",
    "        val_psnr_values_srcnn += psnr_srcnn\n",
    "\n",
    "        val_ssim_values_vdsr += ssim_vdsr\n",
    "        val_ssim_values_srcnn += ssim_srcnn\n",
    "\n",
    "    # T√≠nh PSNR v√† SSIM trung b√¨nh cho VDSR v√† SRCNN\n",
    "    val_average_psnr_vdsr = val_psnr_values_vdsr / len(valid_loader_2)\n",
    "    val_average_psnr_srcnn = val_psnr_values_srcnn / len(valid_loader_2)\n",
    "\n",
    "    val_average_ssim_vdsr = val_ssim_values_vdsr / len(valid_loader_2)\n",
    "    val_average_ssim_srcnn = val_ssim_values_srcnn / len(valid_loader_2)\n",
    "\n",
    "    # Th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho VDSR v√† SRCNN\n",
    "    avg_time_vdsr = vdsr_total_time / len(valid_loader_2)\n",
    "    avg_time_srcnn = srcnn_total_time / len(valid_loader_2)\n",
    "\n",
    "    # Ghi k·∫øt qu·∫£ v√†o log_file\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():  # Kh√¥ng t√≠nh to√°n gradient trong qu√° tr√¨nh validation\n",
    "    for (lr_images, hr_images) in tqdm(valid_loader, unit='batch'):\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "\n",
    "        # Sobel SR validation (kh√¥ng t√≠nh loss, ch·ªâ t√≠nh PSNR v√† SSIM)\n",
    "        start_time = time.time()  # ƒêo th·ªùi gian b·∫Øt ƒë·∫ßu\n",
    "        outputs_sobel = edrn_sobel(lr_images)\n",
    "        sobel_total_time += time.time() - start_time  # T√≠nh th·ªùi gian x·ª≠ l√Ω\n",
    "        psnr_sobel, ssim_sobel = calculate_metrics(outputs_sobel, hr_images)\n",
    "\n",
    "        #bicubic\n",
    "        start_time = time.time()  # ƒêo th·ªùi gian b·∫Øt ƒë·∫ßu\n",
    "        outputs_bicubic = F.interpolate(lr_images, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        bicubic_total_time += time.time() - start_time  # T√≠nh th·ªùi gian x·ª≠ l√Ω bicubic\n",
    "        psnr_bicubic, ssim_bicubic = calculate_metrics(outputs_bicubic, hr_images)\n",
    "\n",
    "        # Canny SR validation\n",
    "        start_time = time.time()\n",
    "        outputs_canny = edrn_canny(lr_images)\n",
    "        canny_total_time += time.time() - start_time\n",
    "        psnr_canny, ssim_canny = calculate_metrics(outputs_canny, hr_images)\n",
    "\n",
    "        # EDSR SR validation\n",
    "        start_time = time.time()\n",
    "        outputs_edsr = edsr(lr_images)\n",
    "        edsr_total_time += time.time() - start_time\n",
    "        psnr_edsr, ssim_edsr = calculate_metrics(outputs_edsr, hr_images)\n",
    "\n",
    "        # SRResNet SR validation\n",
    "        start_time = time.time()\n",
    "        outputs_srresnet = srresnet(lr_images)\n",
    "        srresnet_total_time += time.time() - start_time\n",
    "        psnr_srresnet, ssim_srresnet = calculate_metrics(outputs_srresnet, hr_images)\n",
    "\n",
    "        # C·∫≠p nh·∫≠t PSNR v√† SSIM\n",
    "        val_psnr_values_sobel += psnr_sobel\n",
    "        val_psnr_values_canny += psnr_canny\n",
    "        val_psnr_values_edsr += psnr_edsr\n",
    "        val_psnr_values_srresnet += psnr_srresnet\n",
    "        val_psnr_values_bicubic += psnr_bicubic\n",
    "        \n",
    "        val_ssim_values_sobel += ssim_sobel\n",
    "        val_ssim_values_canny += ssim_canny\n",
    "        val_ssim_values_edsr += ssim_edsr\n",
    "        val_ssim_values_srresnet += ssim_srresnet\n",
    "        val_ssim_values_bicubic += ssim_bicubic\n",
    "    # T√≠nh PSNR v√† SSIM trung b√¨nh\n",
    "    val_average_psnr_sobel = val_psnr_values_sobel / len(valid_loader)\n",
    "    val_average_psnr_canny = val_psnr_values_canny / len(valid_loader)\n",
    "    val_average_psnr_edsr = val_psnr_values_edsr / len(valid_loader)\n",
    "    val_average_psnr_srresnet = val_psnr_values_srresnet / len(valid_loader)\n",
    "    val_average_psnr_bicubic = val_psnr_values_bicubic / len(valid_loader)\n",
    "    \n",
    "    val_average_ssim_sobel = val_ssim_values_sobel / len(valid_loader)\n",
    "    val_average_ssim_canny = val_ssim_values_canny / len(valid_loader)\n",
    "    val_average_ssim_edsr = val_ssim_values_edsr / len(valid_loader)\n",
    "    val_average_ssim_srresnet = val_ssim_values_srresnet / len(valid_loader)\n",
    "    val_average_ssim_bicubic = val_ssim_values_bicubic / len(valid_loader)\n",
    "\n",
    "    # Th·ªùi gian x·ª≠ l√Ω trung b√¨nh\n",
    "    avg_time_sobel = sobel_total_time / len(valid_loader)\n",
    "    avg_time_canny = canny_total_time / len(valid_loader)\n",
    "    avg_time_edsr = edsr_total_time / len(valid_loader)\n",
    "    avg_time_srresnet = srresnet_total_time / len(valid_loader)\n",
    "    avg_time_bicubic = bicubic_total_time / len(valid_loader)\n",
    "    \n",
    "\n",
    "    # Th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho Bicubic\n",
    "    \n",
    "    temp_psnr = val_average_psnr_bicubic\n",
    "    temp_ssim = val_average_ssim_bicubic\n",
    "    # Ghi k·∫øt qu·∫£ v√†o log_file\n",
    "    # Validation cho VDSR v√† SRCNN\n",
    "log_file.write(f'Bicubic:  SSIM / PSNR {val_average_ssim_bicubic:.4f} / {val_average_psnr_bicubic:.2f}, Time {avg_time_bicubic:.4f}s\\n')\n",
    "log_file.write(f'VDSR:  SSIM / PSNR {val_average_ssim_vdsr:.4f} / {val_average_psnr_vdsr:.2f}, Time {avg_time_vdsr:.4f}s\\n')\n",
    "log_file.write(f'SRCNN:  SSIM / PSNR {val_average_ssim_srcnn:.4f} / {val_average_psnr_srcnn:.2f}, Time {avg_time_srcnn:.4f}s\\n')    \n",
    "log_file.write(f'SRResNet:  SSIM / PSNR {val_average_ssim_srresnet:.4f} / {val_average_psnr_srresnet:.2f}, Time {avg_time_srresnet:.4f}s\\n')\n",
    "log_file.write(f'EDSR:  SSIM / PSNR {val_average_ssim_edsr:.4f} / {val_average_psnr_edsr:.2f}, Time {avg_time_edsr:.4f}s\\n')\n",
    "log_file.write(f'Sobel:  SSIM / PSNR {val_average_ssim_sobel:.4f} / {val_average_psnr_sobel:.2f}, Time {avg_time_sobel:.4f}s\\n')\n",
    "log_file.write(f'Canny:  SSIM / PSNR {val_average_ssim_canny:.4f} / {val_average_psnr_canny:.2f}, Time {avg_time_canny:.4f}s\\n')\n",
    "\n",
    "log_file.flush()\n",
    "log_file.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'benchmark/pcb/LR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# index = random.randint(0, len(os.listdir(lr_image_dir))-1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# file = '0875.png'\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# lr_image_file = file\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# hr_image_file = file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (lr_image_file, hr_image_file) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_image_dir\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(hr_image_dir))):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         lr_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(lr_image_dir, lr_image_file)\n\u001b[1;32m     23\u001b[0m         hr_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hr_image_dir, hr_image_file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'benchmark/pcb/LR'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from PIL import ImageDraw\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "sub = 'pcb'\n",
    "box = 80\n",
    "valid_lr_dir = f'benchmark/{sub}/LR'\n",
    "valid_hr_dir = f'benchmark/{sub}/HR'\n",
    "output_image_dir = f'benchmark/output/{sub}'\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "lr_image_dir = valid_lr_dir\n",
    "hr_image_dir = valid_hr_dir\n",
    "# index = random.randint(0, len(os.listdir(lr_image_dir))-1)\n",
    "# file = '0875.png'\n",
    "# lr_image_file = file\n",
    "# hr_image_file = file\n",
    "with torch.no_grad():\n",
    "    for (lr_image_file, hr_image_file) in zip(sorted(os.listdir(lr_image_dir)), sorted(os.listdir(hr_image_dir))):\n",
    "        # ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh\n",
    "\n",
    "        lr_image_path = os.path.join(lr_image_dir, lr_image_file)\n",
    "        hr_image_path = os.path.join(hr_image_dir, hr_image_file)\n",
    "        ####################################\n",
    "        img = cv2.imread(hr_image_path)  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n ·∫£nh c·ªßa b·∫°n\n",
    "        cv2.namedWindow(\"Image\")\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        drawing = False  # True khi ƒëang nh·∫•n chu·ªôt\n",
    "        top_left_corner = [0,0]\n",
    "        xy = [0, 0, 0, 0]\n",
    "        def draw_rectangle(event, x, y, flags, param):\n",
    "            global drawing, top_left_corner, img, xy\n",
    "            \n",
    "            # Khi nh·∫•n chu·ªôt tr√°i, b·∫Øt ƒë·∫ßu v·∫Ω\n",
    "            if event == cv2.EVENT_LBUTTONDOWN:\n",
    "                drawing = True\n",
    "                top_left_corner = [x, y]   # L∆∞u t·ªça ƒë·ªô c·ªßa ƒëi·ªÉm b·∫Øt ƒë·∫ßu\n",
    "            \n",
    "            # Khi th·∫£ chu·ªôt tr√°i, k·∫øt th√∫c v·∫Ω\n",
    "            elif event == cv2.EVENT_LBUTTONUP:\n",
    "                drawing = False\n",
    "                \n",
    "                xy[0] = top_left_corner[0]\n",
    "                xy[1] = top_left_corner[1]\n",
    "                xy[2] = top_left_corner[0] + box # type: ignore\n",
    "                xy[3] = top_left_corner[1] + box\n",
    "                \n",
    "                cv2.destroyAllWindows()\n",
    "                # ƒê√≥ng c·ª≠a s·ªï ·∫£nh sau khi v·∫Ω xong\n",
    "        # ƒê·∫∑t callback cho s·ª± ki·ªán chu·ªôt\n",
    "        cv2.setMouseCallback(\"Image\", draw_rectangle)\n",
    "\n",
    "        # Hi·ªÉn th·ªã ·∫£nh v√† ƒë·ª£i cho ƒë·∫øn khi bounding box ƒë∆∞·ª£c v·∫Ω\n",
    "        cv2.waitKey(0)\n",
    "        ####################################\n",
    "        edsr_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_edsr.jpg')\n",
    "        edrn_sobel_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_edrn_sobel.jpg')\n",
    "        edrn_canny_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_edrn_canny.jpg')\n",
    "        bicubic_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_bicubic.jpg')\n",
    "        srresnet_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_srresnet.jpg')\n",
    "        srcnn_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_srcnn.jpg')\n",
    "        vdsr_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_vdsr.jpg')\n",
    "\n",
    "        hr_path = os.path.join(output_image_dir, hr_image_file)\n",
    "        # T·∫£i v√† chuy·ªÉn ƒë·ªïi ·∫£nh\n",
    "        lr_image = Image.open(lr_image_path)\n",
    "        hr_image = Image.open(hr_image_path)\n",
    "        w, h = hr_image.size\n",
    "        bicubic = lr_image.resize((w, h))\n",
    "        lr_image_copy = lr_image.copy().convert('RGB')\n",
    "        hr_image_copy = hr_image.copy()\n",
    "        # lr_image_1dms = torch.Tensor(lr_image_1dms)\n",
    "        lr_image = transform(lr_image).unsqueeze(0).to(device)  # Th√™m batch dimension v√† chuy·ªÉn sang CPU\n",
    "        hr_image = transform(hr_image).unsqueeze(0).to(device)  # Th√™m batch dimension v√† chuy·ªÉn sang CPU\n",
    "        bicubic_ = transform(bicubic).unsqueeze(0).to(device)\n",
    "        # print(type(lr_image_1dms))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # bicubic = lr_image_copy.resize((600, 600), resample=Image.BICUBIC) # type: ignore\n",
    "        # bicubic_ = transform(bicubic).unsqueeze(0).to(device)\n",
    "        # lr_image_1dms, _ = preprocess(lr_image_copy, device)\n",
    "        # lr_image_copy, _ = preprocess(lr_image_copy, device)\n",
    "        # hr_image_copy, _ = preprocess(HR, device)\n",
    "        # _, ycbcr = preprocess(bicubic, device)\n",
    "        time_pro = []\n",
    "        def measure_inference_time(model, input_image, model_name):\n",
    "            start_time = time.time()\n",
    "            output = model(input_image)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_time = end_time - start_time\n",
    "            time_pro.append(inference_time)\n",
    "            return output\n",
    "        \n",
    "        # D·ª± ƒëo√°n\n",
    "        output_edsr = edsr(lr_image)\n",
    "        output_edrn_sobel = edrn_sobel(lr_image)\n",
    "        output_edrn_canny = edrn_canny(lr_image)\n",
    "        output_srresnet = srresnet(lr_image)\n",
    "        output_srcnn = srcnn(bicubic_)\n",
    "        output_vdsr = vdsr(bicubic_)\n",
    "        models = ['EDSR', 'EDRN Sobel', 'EDRN Canny', 'SRResNet', 'SRCNN', 'VDSR']\n",
    "        psnr_value = []  # L∆∞u gi√° tr·ªã PSNR cho m·ªói m√¥ h√¨nh\n",
    "        ssim_value = []\n",
    "        # T√≠nh PSNR cho t·ª´ng m√¥ h√¨nh (gi·∫£ s·ª≠ output c·ªßa c√°c m√¥ h√¨nh ƒë√£ c√≥)\n",
    "        for model_output in [output_edsr, output_edrn_sobel, output_edrn_canny, output_srresnet, output_srcnn, output_vdsr]:\n",
    "            # T√≠nh PSNR v√† th√™m v√†o danh s√°ch\n",
    "            psnr_value.append(calculate_metrics(model_output, hr_image)[0])\n",
    "            ssim_value.append(calculate_metrics(model_output, hr_image)[1])\n",
    "        # Ghi PSNR cho t·ª´ng m√¥ h√¨nh v√†o file\n",
    "        with open(f\"{output_image_dir}/results.txt\", \"a\") as psnr_file:\n",
    "            psnr_file.write(f\"HR Image: {hr_image_file}\\n\")\n",
    "            for i, model_name in enumerate(models):\n",
    "                psnr_file.write(f\"{model_name} PSNR/SSIM: {psnr_value[i]:.2f} dB/ {ssim_value[i]:.4f}\\n\")\n",
    "            psnr_file.write(f\"Bicubic PSNR/SSIM: {calculate_metrics(bicubic_, hr_image)[0]:.2f} dB/ {calculate_metrics(bicubic_, hr_image)[1]:.2f}\\n\")\n",
    "            psnr_file.write(\"\\n\")  # D√≤ng tr·ªëng gi·ªØa c√°c l·∫ßn l·∫∑p\n",
    "            \n",
    "            \n",
    "        output_image_edsr = output_edsr.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image_edsr = transforms.ToPILImage()(output_image_edsr)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image_edsr = output_image_edsr.crop(xy)\n",
    "        output_image_edsr.resize((100, 100))\n",
    "        output_image_edsr.save(edsr_path)  # L∆∞u ·∫£nh\n",
    "        \n",
    "        output_image_edrn_sobel = output_edrn_sobel.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image_edrn_sobel = transforms.ToPILImage()(output_image_edrn_sobel)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image_edrn_sobel = output_image_edrn_sobel.crop(xy)\n",
    "        output_image_edrn_sobel.resize((100, 100))\n",
    "        output_image_edrn_sobel.save(edrn_sobel_path)  # L∆∞u ·∫£nh\n",
    "        \n",
    "        \n",
    "        output_image_edrn_canny = output_edrn_canny.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image_edrn_canny = transforms.ToPILImage()(output_image_edrn_canny)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image_edrn_canny = output_image_edrn_canny.crop(xy)\n",
    "        output_image_edrn_canny.resize((100, 100))\n",
    "        output_image_edrn_canny.save(edrn_canny_path)  # L∆∞u ·∫£nh\n",
    "        \n",
    "        output_image_srcnn = output_srcnn.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image_srcnn = transforms.ToPILImage()(output_image_srcnn)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image_srcnn = output_image_srcnn.crop(xy)\n",
    "        output_image_srcnn.resize((100, 100))\n",
    "        output_image_srcnn.save(srcnn_path)  # L∆∞u ·∫£nh\n",
    "        \n",
    "        # output_vdsr = output_vdsr.mul(255.0).to(device).numpy().squeeze(0).squeeze(0)\n",
    "        # output_image_vdsr = np.array([output_vdsr, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n",
    "        # output_image_vdsr = np.clip(convert_ycbcr_to_rgb(output_image_vdsr ), 0.0, 255.0).astype(np.uint8)\n",
    "        # output_image_vdsr = Image.fromarray(output_image_vdsr )\n",
    "        # output_image_vdsr = output_image_vdsr.crop(xy)\n",
    "        # output_image_vdsr .save(vdsr_path) \n",
    "        \n",
    "        output_image_vdsr = output_vdsr.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image_vdsr = transforms.ToPILImage()(output_image_vdsr)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image_vdsr = output_image_vdsr.crop(xy)\n",
    "        output_image_vdsr.resize((100, 100))\n",
    "        output_image_vdsr.save(vdsr_path)  # L∆∞u ·∫£n\n",
    "        \n",
    "        output_image_srresnet = output_srresnet.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image_srresnet = transforms.ToPILImage()(output_image_srresnet)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image_srresnet = output_image_srresnet.crop(xy)\n",
    "        output_image_srresnet.resize((100, 100))\n",
    "        output_image_srresnet.save(srresnet_path)  # L∆∞u ·∫£nh\n",
    "        \n",
    "        # bicubic = lr_image.resize((600, 600))\n",
    "        bicubic = bicubic.crop(xy)\n",
    "        bicubic.resize((100, 100))\n",
    "        bicubic.save(bicubic_path)\n",
    "        \n",
    "        hr_image_crop = hr_image_copy.crop(xy)\n",
    "        hr_image_crop.resize((100, 100))\n",
    "        hr_image_crop.save(hr_path)\n",
    "        \n",
    "        draw = ImageDraw.Draw(hr_image_copy)\n",
    "        draw.rectangle(xy, outline=\"red\", width=3)\n",
    "        hr_image_copy = hr_image_copy.resize((600, 600))\n",
    "        hr_image_copy.save(f\"{output_image_dir}/{hr_image_file[:-4]}_with_bounding_box.jpg\")\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0img [00:00, ?img/s]/home/robot/anaconda3/envs/pcb/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/home/robot/anaconda3/envs/pcb/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "1068img [05:36,  3.17img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRResNet\n",
      "\n",
      "missinghole_psnr: 26.71\n",
      "\n",
      "mousebite_psnr: 26.78\n",
      "\n",
      "opencircuit_psnr: 26.83\n",
      "\n",
      "short_psnr: 26.72\n",
      "\n",
      "spur_psnr: 26.88\n",
      "\n",
      "spuriouscopper_psnr: 26.64\n",
      "\n",
      "average_psnr: 26.76\n",
      "\n",
      "time process: 336.76\n",
      "\n",
      "missinghole_ssim: 0.75\n",
      "\n",
      "mousebite_ssim: 0.75\n",
      "\n",
      "opencircuit_ssim: 0.75\n",
      "\n",
      "short_ssim: 0.75\n",
      "\n",
      "spur_ssim: 0.76\n",
      "\n",
      "spuriouscopper_ssim: 0.75\n",
      "\n",
      "average_ssim: 0.75\n",
      "\n",
      "time process: 336.76\n",
      "\n",
      "ƒê√£ sao ch√©p c√°c file th√†nh c√¥ng.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for (sub, model) in zip(['VDSR', 'SRCNN'], [vdsr, srcnn]):\n",
    "sub = 'SRResNet'\n",
    "model = srresnet\n",
    "lr_image_dir = f'test1_600x600/images'\n",
    "hr_image_dir = f'test1_600x600/images'\n",
    "output_image_dir = f'output/{sub}/images'\n",
    "os.makedirs(output_image_dir, exist_ok = True)\n",
    "# Duy·ªát qua c√°c ·∫£nh trong th∆∞ m·ª•c\n",
    "lr_image_files = os.listdir(lr_image_dir)\n",
    "hr_image_files = os.listdir(hr_image_dir)\n",
    "lr_image_files.sort()\n",
    "hr_image_files.sort()\n",
    "\n",
    "psnr_dict = {\n",
    "    \"mouse_bite\" :[],\n",
    "    \"spur_\":[], \n",
    "    \"missing_hole\":[],\n",
    "    \"short\":[],\n",
    "    \"open_circuit\":[],\n",
    "    \"spurious_copper\":[]\n",
    "\n",
    "}\n",
    "ssim_dict = {\n",
    "    \"mouse_bite\" :[],\n",
    "    \"spur_\":[], \n",
    "    \"missing_hole\":[],\n",
    "    \"short\":[],\n",
    "    \"open_circuit\":[],\n",
    "    \"spurious_copper\":[]\n",
    "\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "transform = transforms.ToTensor()\n",
    "with torch.no_grad():\n",
    "    for lr_image_file, hr_image_file in tqdm(zip(lr_image_files, hr_image_files), unit = 'img'):\n",
    "        # ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh\n",
    "        lr_image_path = os.path.join(lr_image_dir, lr_image_file)\n",
    "        hr_image_path = os.path.join(hr_image_dir, hr_image_file)\n",
    "        output_image_path = os.path.join(output_image_dir, lr_image_file)\n",
    "\n",
    "        # T·∫£i v√† chuy·ªÉn ƒë·ªïi ·∫£nh\n",
    "        lr_image = Image.open(lr_image_path)\n",
    "        hr_image = Image.open(hr_image_path)\n",
    "        lr_image = lr_image.resize((150, 150))\n",
    "        # lr_image = lr_image.resize((600, 600))\n",
    "        lr_image = transform(lr_image).unsqueeze(0).to(device)  # Th√™m batch dimension v√† chuy·ªÉn sang CPU\n",
    "        hr_image = transform(hr_image).unsqueeze(0).to(device)  # Th√™m batch dimension v√† chuy·ªÉn sang CPU\n",
    "\n",
    "        # D·ª± ƒëo√°n\n",
    "        output = model(lr_image)\n",
    "\n",
    "        # T√≠nh to√°n PSNR\n",
    "        psnr,ssim = calculate_metrics(output, hr_image)\n",
    "        for key in psnr_dict.keys():\n",
    "            if key in lr_image_path:\n",
    "                psnr_dict[key].append(psnr)\n",
    "                ssim_dict[key].append(ssim)\n",
    "                break\n",
    "\n",
    "        # Chuy·ªÉn ƒë·ªïi tensor ƒë·∫ßu ra th√†nh ·∫£nh v√† l∆∞u\n",
    "        output_image = output.squeeze(0).to(device)  # Lo·∫°i b·ªè batch dimension v√† chuy·ªÉn tensor sang CPU\n",
    "        output_image = transforms.ToPILImage()(output_image)  # Chuy·ªÉn tensor th√†nh ·∫£nh PIL\n",
    "        output_image.save(output_image_path)  # L∆∞u ·∫£nh\n",
    "avg_psnr = [0, 0, 0, 0, 0, 0]\n",
    "avg_ssim = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "# T√≠nh to√°n PSNR trung b√¨nh\n",
    "avg_psnr[2] = sum(psnr_dict['missing_hole'])/len(psnr_dict['missing_hole']) #missinghole_psnr \n",
    "avg_psnr[0] = sum(psnr_dict['mouse_bite'])/len(psnr_dict['mouse_bite']) #mousebite_psnr\n",
    "avg_psnr[4] = sum(psnr_dict['open_circuit'])/len(psnr_dict['open_circuit']) #opencircuit_psnr\n",
    "avg_psnr[3] = sum(psnr_dict['short'])/len(psnr_dict['short']) #short_psnr\n",
    "avg_psnr[1] = sum(psnr_dict['spur_'])/len(psnr_dict['spur_']) #spur_psnr\n",
    "avg_psnr[5]= sum(psnr_dict['spurious_copper'])/len(psnr_dict['spurious_copper']) #spuriouscopper_psnr \n",
    "average_psnr = sum(avg_psnr)/len(avg_psnr)\n",
    "avg_ssim[2] = sum(ssim_dict['missing_hole'])/len(ssim_dict['missing_hole']) #missinghole_ssim \n",
    "avg_ssim[0] = sum(ssim_dict['mouse_bite'])/len(ssim_dict['mouse_bite']) #mousebite_ssim\n",
    "avg_ssim[4] = sum(ssim_dict['open_circuit'])/len(ssim_dict['open_circuit']) #opencircuit_ssim\n",
    "avg_ssim[3] = sum(ssim_dict['short'])/len(ssim_dict['short']) #short_ssim\n",
    "avg_ssim[1] = sum(ssim_dict['spur_'])/len(ssim_dict['spur_']) #spur_ssim\n",
    "avg_ssim[5]= sum(ssim_dict['spurious_copper'])/len(ssim_dict['spurious_copper']) #spuriouscopper_ssim  \n",
    "average_ssim = sum(avg_ssim)/len(avg_ssim)\n",
    "end = time.time()\n",
    "\n",
    "with open('results.txt', 'a') as f:\n",
    "    f.write(output_image_dir.split('/')[1] + '\\n')\n",
    "    f.write(f'missinghole_psnr: {avg_psnr[2]:.2f}' + '\\n')\n",
    "    f.write(f'mousebite_psnr: {avg_psnr[0]:.2f}' + '\\n')\n",
    "    f.write(f'opencircuit_psnr: {avg_psnr[4]:.2f}' + '\\n')\n",
    "    f.write(f'short_psnr: {avg_psnr[3]:.2f}' + '\\n')\n",
    "    f.write(f'spur_psnr: {avg_psnr[1]:.2f}' + '\\n')\n",
    "    f.write(f'spuriouscopper_psnr: {avg_psnr[5]:.2f}' + '\\n')\n",
    "    f.write(f'average_psnr: {average_psnr:.2f}' + '\\n')\n",
    "    f.write(f'time process: {end - start:.2f}' + '\\n')\n",
    "\n",
    "    f.write(output_image_dir.split('/')[1] + '\\n')\n",
    "    f.write(f'missinghole_psnr: {avg_ssim[2]:.2f}' + '\\n')\n",
    "    f.write(f'mousebite_psnr: {avg_ssim[0]:.2f}' + '\\n')\n",
    "    f.write(f'opencircuit_psnr: {avg_ssim[4]:.2f}' + '\\n')\n",
    "    f.write(f'short_psnr: {avg_ssim[3]:.2f}' + '\\n')\n",
    "    f.write(f'spur_psnr: {avg_ssim[1]:.2f}' + '\\n')\n",
    "    f.write(f'spuriouscopper_psnr: {avg_ssim[5]:.2f}' + '\\n')\n",
    "    f.write(f'average_psnr: {average_ssim:.2f}' + '\\n')\n",
    "    f.write(f'time process: {end - start:.2f}' + '\\n')\n",
    "\n",
    "\n",
    "# t·∫°o d·ªØ li·ªáu cho YOLO\n",
    "print(output_image_dir.split('/')[1] + '\\n')\n",
    "print(f'missinghole_psnr: {avg_psnr[2]:.2f}' + '\\n')\n",
    "print(f'mousebite_psnr: {avg_psnr[0]:.2f}' + '\\n')\n",
    "print(f'opencircuit_psnr: {avg_psnr[4]:.2f}' + '\\n')\n",
    "print(f'short_psnr: {avg_psnr[3]:.2f}' + '\\n')\n",
    "print(f'spur_psnr: {avg_psnr[1]:.2f}' + '\\n')\n",
    "print(f'spuriouscopper_psnr: {avg_psnr[5]:.2f}' + '\\n')\n",
    "print(f'average_psnr: {average_psnr:.2f}' + '\\n')\n",
    "print(f'time process: {end - start:.2f}' + '\\n')\n",
    "print(f'missinghole_ssim: {avg_ssim[2]:.2f}' + '\\n')\n",
    "print(f'mousebite_ssim: {avg_ssim[0]:.2f}' + '\\n')\n",
    "print(f'opencircuit_ssim: {avg_ssim[4]:.2f}' + '\\n')\n",
    "print(f'short_ssim: {avg_ssim[3]:.2f}' + '\\n')\n",
    "print(f'spur_ssim: {avg_ssim[1]:.2f}' + '\\n')\n",
    "print(f'spuriouscopper_ssim: {avg_ssim[5]:.2f}' + '\\n')\n",
    "print(f'average_ssim: {average_ssim:.2f}' + '\\n')\n",
    "print(f'time process: {end - start:.2f}' + '\\n')\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ngu·ªìn v√† ƒë√≠ch\n",
    "source_dir = f'test1_600x600/labels'\n",
    "\n",
    "dest = f'output/{sub}/labels'\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c ƒë√≠ch n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "# Sao ch√©p c√°c file t·ª´ th∆∞ m·ª•c ngu·ªìn sang th∆∞ m·ª•c ƒë√≠ch\n",
    "for filename in os.listdir(source_dir):\n",
    "    source_file = os.path.join(source_dir, filename)\n",
    "    file1 = os.path.join(dest, filename)\n",
    "\n",
    "    shutil.copy(source_file, file1)\n",
    "\n",
    "print(\"ƒê√£ sao ch√©p c√°c file th√†nh c√¥ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File config.yaml ƒë√£ ƒë∆∞·ª£c t·∫°o v√† ghi th√†nh c√¥ng.\n",
      "WARNING ‚ö†Ô∏è imgsz=[600] must be multiple of max stride 32, updating to [608]\n",
      "Ultralytics 8.3.9 üöÄ Python-3.12.4 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 10971MiB)\n",
      "Model summary (fused): 168 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/robot/Desktop/pcb/output/SRResNet/labels... 1068 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1068/1068 [00:00<00:00, 2440.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/robot/Desktop/pcb/output/SRResNet/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1068/1068 [00:05<00:00, 198.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1068       2158      0.914      0.759      0.837      0.517\n",
      "            mouse_bite        168        332      0.894      0.762      0.811      0.488\n",
      "                  spur        169        348      0.739      0.787      0.789      0.491\n",
      "          missing_hole        190        379      0.987      0.842      0.932      0.602\n",
      "                 short        184        366      0.967       0.73      0.845      0.541\n",
      "          open_circuit        166        345      0.963       0.84      0.911      0.538\n",
      "       spurious_copper        191        388      0.932      0.593      0.737      0.443\n",
      "Speed: 0.5ms preprocess, 2.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val38\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.48829,     0.49108,     0.60177,     0.54134,     0.53768,     0.44313])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# N·ªôi dung c·ªßa file YAML\n",
    "# sub = 'VDSR'\n",
    "data = {\n",
    "    'train': f'/home/robot/Desktop/pcb/test1_600x600',\n",
    "    'val': f'./{sub}/images',\n",
    "    'nc': 6,\n",
    "    'names': {\n",
    "        0: 'mouse_bite',\n",
    "        1: 'spur',\n",
    "        2: 'missing_hole',\n",
    "        3: 'short',\n",
    "        4: 'open_circuit',\n",
    "        5: 'spurious_copper'\n",
    "    }\n",
    "}\n",
    "\n",
    "# T·∫°o v√† ghi file YAML\n",
    "with open('output/data.yaml', 'w') as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "print(\"File config.yaml ƒë√£ ƒë∆∞·ª£c t·∫°o v√† ghi th√†nh c√¥ng.\")\n",
    "\n",
    "\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolov8n.pt\")  # load an official model\n",
    "model = YOLO(\"bestweight_2006.pt\")  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val(data = 'output/data.yaml', batch = 1, imgsz = 600)  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
